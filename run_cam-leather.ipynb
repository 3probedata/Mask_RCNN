{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leather Detection for Mask RCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "ROOT:  D:\\Project\\Mask_RCNN\n",
      "D:\\Project\\Mask_RCNN\\logs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import imutils\n",
    "import numpy as np\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import utils, visualize\n",
    "from imutils.video import WebcamVideoStream\n",
    "import random\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "import datetime\n",
    "\n",
    "# Root directory of the project\n",
    "from mrcnn.config import Config\n",
    "# 根目錄\n",
    "ROOT_DIR = os.path.abspath(\"./\")\n",
    "# 系統路徑設定\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"samples/leather/\"))  # To find local version\n",
    "\n",
    "out_filename = '' # video output file name\n",
    "# Directory to save logs and trained model\n",
    "# 模型儲存目錄\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\") \n",
    "print(MODEL_DIR)\n",
    "#video_input = \"D:/Project/yolov4/Mango1/data/data/2019_tw_run.mp4\" #\"video source. If video_input=0, uses webcam 0 stream\"\n",
    "\n",
    "# 鏡頭選擇\n",
    "video_input = 0 #usb cam device 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定權重檔路徑(Weights file)\n",
    "## 將Config導入，預設值來自於mrcnn/config.py 以及本地目錄中的config.ini，如果Mask RCNN已經安裝到虛擬環境，則需要到虛擬環境下的`<envs>\\Lib\\site-packages\\mask_rcnn-2.1-py3.7.egg`\n",
    "### 打包[egg](https://blog.csdn.net/caiguoxiong0101/article/details/50285279) ，完成後將Mask RCNN目錄 `dist/`下面的 *.egg 覆蓋到安裝路徑上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO_MODEL_PATH: PASS D:\\Project\\Mask_RCNN\\logs\\shapes20201218T1013\\mask_rcnn_shapes_0150.h5\n",
      "\n",
      "Configurations:\n",
      "AUGMENT                        False\n",
      "AUGMENTATION                   False\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  448\n",
      "IMAGE_META_SIZE                22\n",
      "IMAGE_MIN_DIM                  384\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [448 448   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    10\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (48, 96, 192, 384, 768)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           100\n",
      "USE_MINI_MASK                  False\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "config                         <configparser.ConfigParser object at 0x0000020F95E8BC88>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(MODEL_DIR, 'shapes20201218T1013',\"mask_rcnn_shapes_0150.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "#if not os.path.exists(COCO_MODEL_PATH):\n",
    "#    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "if os.path.exists(COCO_MODEL_PATH):\n",
    "    print('COCO_MODEL_PATH: PASS', COCO_MODEL_PATH)\n",
    "\n",
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 9  # background + 1 shapes #如果你有超過一個label, 記得改1 + 2或者以上\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 384\n",
    "    IMAGE_MAX_DIM = 448\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8 * 6, 16 * 6, 32 * 6, 64 * 6, 128 * 6)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 100\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 50\n",
    "\n",
    "\n",
    "config = ShapesConfig()\n",
    "onfig.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型Mask RCNN導入\n",
    "## 判斷畫面上的瑕疵並將結果輸出內容顯示在畫面上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN_NAME = 'Mask RCNN LIVE' # CV2 Windw name\n",
    "# CV2全螢幕顯示\n",
    "#cv2.namedWindow(SCREEN_NAME, cv2.WINDOW_NORMAL)\n",
    "#cv2.setWindowProperty(SCREEN_NAME, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "\n",
    "#width = config.IMAGE_MAX_DIM\n",
    "#height = config.IMAGE_MIN_DIM\n",
    "\n",
    "def CapVideo(frame):\n",
    "\n",
    "    #if SHOW_FPS_WO_COUNTER:\n",
    "    start_time = time.time()  # start time of the loop\n",
    "    cv2.imshow('Before', frame)\n",
    "    \n",
    "    if PROCESS_IMG:\n",
    "        results = model.detect([frame])\n",
    "        r = results[0]\n",
    "\n",
    "        # Run detection\n",
    "        masked_image = visualize.display_instances_10fps(frame, r['rois'], r['masks'],\n",
    "                                                         r['class_ids'], class_names, r['scores'], colors=colors,\n",
    "                                                         real_time=True)\n",
    "    \n",
    "\n",
    "    if PROCESS_IMG:\n",
    "        s = masked_image\n",
    "    else:\n",
    "        s = frame\n",
    "    # print(\"Image shape: {1}x{0}\".format(s.shape[0], s.shape[1]))\n",
    "\n",
    "    width = s.shape[1]\n",
    "    height = s.shape[0]\n",
    "    top_left_corner = (width - 120, height - 20)\n",
    "    bott_right_corner = (width, height)\n",
    "    top_left_corner_cvtext = (width - 80, height - 5)\n",
    "\n",
    "    if SHOW_FPS:\n",
    "        fps_counter += 1\n",
    "        if (time.time() - start_time) > 5:  # every 5 second\n",
    "            fps_caption = \"FPS: {:.0f}\".format(fps_counter / (time.time() - start_time))\n",
    "            # print(fps_caption)\n",
    "            fps_counter = 0\n",
    "            start_time = time.time()\n",
    "        ret, baseline = cv2.getTextSize(fps_caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "        cv2.rectangle(s, (width - ret[0], height - ret[1] - baseline), bott_right_corner, gentle_grey, -1)\n",
    "        cv2.putText(s, fps_caption, (width - ret[0], height - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.5, white,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "\n",
    "    if SHOW_FPS_WO_COUNTER:\n",
    "        # Display the resulting frame\n",
    "        fps_caption = \"FPS: {:.0f}\".format(1.0 / (time.time() - start_time))\n",
    "        # print(\"FPS: \", 1.0 / (time.time() - start_time))\n",
    "\n",
    "        # Put the rectangle and text on the bottom left corner\n",
    "        ret, baseline = cv2.getTextSize(fps_caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "        cv2.rectangle(s, (width - ret[0], height - ret[1] - baseline), bott_right_corner, gentle_grey, -1)\n",
    "        cv2.putText(s, fps_caption, (width - ret[0], height - baseline),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, white, 1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    s = cv2.resize(s, (640, 480))\n",
    "\n",
    "    return SCREEN_NAME, s\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抓取USB輸入影像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoGet:\n",
    "    \"\"\"\n",
    "    Class that continuously gets frames from a VideoCapture object\n",
    "    with a dedicated thread.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src=0):\n",
    "        self.stream = cv2.VideoCapture(src + cv2.CAP_DSHOW)\n",
    "        #self.stream = cv2.VideoCapture(src)\n",
    "        (self.grabbed, self.frame) = self.stream.read()\n",
    "        self.stopped = False\n",
    "        \n",
    "    def start(self):    \n",
    "        Thread(target=self.get, args=()).start()\n",
    "        return self\n",
    "\n",
    "    def get(self):\n",
    "        while not self.stopped:\n",
    "            if not self.grabbed:\n",
    "                self.stop()\n",
    "            else:\n",
    "                (self.grabbed, self.frame) = self.stream.read()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stopped = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭配VideoGet()，將CV2取到的畫面放到Mutil-process處理\n",
    "## 再將影像送到模型中進行判斷，可以加快整體運作速度。\n",
    "## `q`則結束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threadVideoGet(source=0):\n",
    "    \"\"\"\n",
    "    Dedicated thread for grabbing video frames with VideoGet object.\n",
    "    Main thread shows video frames.\n",
    "    \"\"\"\n",
    "\n",
    "    video_getter = VideoGet(source).start()\n",
    "    #cps = CountsPerSec().start()\n",
    "\n",
    "    while True:\n",
    "        if (cv2.waitKey(1) == ord(\"q\")) or video_getter.stopped:\n",
    "            video_getter.stop()\n",
    "            break\n",
    "\n",
    "        frame = video_getter.frame\n",
    "        name, frame = CapVideo(frame)\n",
    "\n",
    "        cv2.imshow(name, frame)\n",
    "        #cps.increment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 程式主流程\n",
    "## 1. 載入模型權重\n",
    "## 2. 定義輸出類別名稱，同訓練時名稱\n",
    "## 3. 將類別顏色使用亂數選擇\n",
    "## 4. 設定參數\n",
    "## 5. 執行threadVideoGet()程式直到`q`被按下\n",
    "## 6. 停止輸入影像\n",
    "## 7. 停止釋放串流與關閉視窗。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-starting from epoch 150\n"
     ]
    }
   ],
   "source": [
    "# Create model object in inference mode.\n",
    "global model\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
    "\n",
    "# COCO Class names\n",
    "# Index of the class in the list is its ID. For example, to get ID of\n",
    "# the teddy bear class, use: class_names.index('teddy bear')\n",
    "class_names = ['BG','scratch', 'scratch2','Wrinkles', 'Wrinkles2','vein', 'vein2',\n",
    "               'GrowthPattern','dot','scratch3']\n",
    "\n",
    "colors = visualize.random_colors(len(class_names))\n",
    "\n",
    "gentle_grey = (45, 65, 79)\n",
    "white = (255, 255, 255)\n",
    "\n",
    "OPTIMIZE_CAM = False\n",
    "SHOW_FPS = False\n",
    "SHOW_FPS_WO_COUNTER = True  # faster\n",
    "PROCESS_IMG = True\n",
    "\n",
    "#if OPTIMIZE_CAM:\n",
    "#    vs = WebcamVideoStream(src=0).start()\n",
    "\n",
    "if SHOW_FPS:\n",
    "    fps_caption = \"FPS: 0\"\n",
    "    global fps_counter\n",
    "    fps_counter = 0\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "\n",
    "threadVideoGet(video_input)\n",
    "video_getter = VideoGet(video_input).start()\n",
    "video_getter.stop()\n",
    "video_getter.stream.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認GPU是否有啟動，某些Jupyter環境啟動後，GPU可能是沒有動作的，但因為Tensorflow2.0之後CPU跟GPU是在同一個版本中，如果沒有特別指定，則是會自動切換。\n",
    "## Mask RCNN若如GPU支援，則判斷速度FPS可能再>1 以下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15058091407120069240\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 15836223316918203098\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5060693856\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14154384174669849841\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9161432300683373942\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "a = tf.constant(1.)\n",
    "b = tf.constant(2.)\n",
    "print(a+b)\n",
    "print(tf.__version__)\n",
    "print('GPU:', tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "print(gpus, cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
